---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - Jeongwoo Kim
  - Jiwon Choi
thanks: "Code and data are available at: https://github.com/Kjeongwoo99/STA302H_Paper3"
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
library(ggrepel)
library(maps)
library(here)
library(kableExtra)
library(statebins)
library(broom.mixed)
library(knitr)

#### Read in necessary data and model ####
cleaned_data_post_strat <-read_csv(file = here("data/analysis_data/usa_00002_cleaned.csv"), show_col_types = FALSE)
cleaned_data_survey <- read_csv(file = here("data/analysis_data/ns20191003_cleaned.csv"), show_col_types = FALSE)
survey_model <- readRDS(file = here("models/consider_trump.rds"))
predicton_model <- readRDS(file = here("models/census_data_with_predictions.rds"))
```

# Introduction

You can and should cross-reference sections and sub-sections. We use @citeR and @rohan.

The remainder of this paper is structured as follows. @sec-data....

# Data {#sec-data}

We have used two datasets for this study. One is the U.S election survey data of Democracy Fund + UCLA Nationscape dataset from the Voter Study Group, conducted on October 3, 2019. Second is the census data from IPUMS America Census Service, which is used as the post-stratification data for the survey data to adjust the weight.

## Survey Data

This survey data is an 18-month election study conducted by UCLA researchers with roughly 6250 online interviews each from from July 2019 to February 2021 (@citeSurveyData). The sample is weighted to represent the U.S. adult population (@citeSurveyData). Nationscape groups weight on the following important factors: gender, the four major census regions, race, Hispanic ethnicity, household income, education, age, language spoken at home, nativity, 2016 presidential vote, and the urban-rural mix of the respondent's ZIP code (@citeSurveyData). According to the data, Male make up 48.3% while female make up 51.3% (@citeSurveyData). 74.2% of the respondents are White, 6.8% are Asian/Pacific, 12% are Black (@citeSurveyData). 20.4% are those between 18-29, 33.4% are 30-49, 32.4% are 50-69, 3.3% are 70+ (@citeSurveyData). On average, 5.1 percent declined immediately among those who are selected for the survey. 16.7 percent of the respondents did not complete the survey. Another 5.9 percent were categorized as speeding or straight-line which means they completed the survey in less than 6 minutes or selected the same response for every question in the three policy question batteries. Leaving these out leave 72.4 percent of the original sample for the analysis.

The Nationscape survey's strength lies in its methodological rigor - the effectiveness in collecting large samples from the U.S. citizen and its weighting strategy desinged to mirror the U.S. adult population by including weight factors such as age, gender, race and income and more. As they filter out inaccurate or missing data, it makes sure that the data collected are accurate and ensures data integrity. While other datasets such as the General Social Survey (GSS) and the American National Election Studies (ANES) are available, the Nationscape dataset's frequency (surveys collected every week) give it an advantage in analyzing electoral trends and shifts in real-time. Its' extensive sample size also justifies the choice of this dataset.

For our analysis, we decided to focus on five demographics: age, gender, education, race and state. Age is important because in general, voters tend to become more conservative as they get older. To account for the age difference, we divided the age group into four categories: 18-29, 30-49, 50-69 and 70+.

Gender is also an important category because in general, men tend be more conservative and women tend to be more liberal. Recently, gender issues are growing social issues and this may affect the election, hence we wanted to explore how this affects our model.

Education is also an interesting factor. In the past, non-college white voters used to support Democrats while college-educated white voters supported Republicans (@Harris). However, there has been a switch in this trend as 61 percent of non-college white voters showed their support wheres just 45 percent of college-educated white voters did in the exit polls (@Harris). Only 37 percent of those without a degree cast their votes for Democrats while 53 percent with a degree did so (@Harris). We categorized education into four categories: 'High school or less', 'Some college', 'College degree', 'Postgrad'.

Race also needs some attention because normally non-white groups are highly in favour of Democrats regardless of candidates and white swing by depending on candidates. According to the statistics collected in 2016, 93% of black, 71% of Latino, 68% of Asian support democrats while only 41% of white support democrats (@Prokop). As white voters make up 74% of the voting population, it is really important for both parties to attain this demographic group.

Lastly, states are very important as some states historically favor conservatives while some states vote for democrats. In general, the west and the east coasts are democrat supporters whereas south are conservative supporters.

## Post-stratification Data

IPUMS ("Integrated Public Use Microdata Series") is a website that offers database of samples of the American population from the American Community Surveys of 2000-present. These samples provide rich qualitative information on the long-term changes in the population. We selected the '2019 ACS' data (@citeIPUMS) as the post-stratification dataset for our research. The ACS is an ongoing survey that collects data monthly, which is then combined into 1-year, 3-year, and 5-year aggregates. It then uses stratified sampling where the U.S population is broken down into sub-groups and initial weights are assigned to each respondent. 

One strength of the IPUMS survey is the fact that it provides a data with detailed demographic of the U.S. population with social, economic and housing characteristics, which is very useful in our analysis of the 2020 U.S presidential election forecast. The longitudinal data of this survey also allows researchers to analyze trends over time. The U.S. Census Bureau offers credibility of the data with high quality checks. The post-stratification process ensures correcting for sampling biases and non-response. On the other hand, since the survey relies on self-report, there lies a risk of response bias inherently. While it is an ongoing survey, there is still a time lag between the data collection and data availability. However, the large sample size, consistency and reliability of the data collection, the integrated data over time with post-stratification can justify the decision to utilize IPUMS data over other sources. 

In processing the raw post-stratification dataset, which initially contained approximately 3.2 million records, we refined it down to about 2.3 million records. This was achieved through a meticulous selection process, ensuring the data's integrity and relevance for our analysis. In our analysis, we've selected the variables 'sex', 'race', 'stateicp', 'age', and 'educd' from the dataset. To simplify our analysis, respondents who indicated 'other' or provided no data for their sex have been excluded. Consequently, 'sex' has been categorized strictly as 'Male' and 'Female'. We've refined 'race' into five categories: 'White', 'Black', 'Asian', 'American Indian', and 'Other', based on the composition of the U.S. population, with White, Black, and Asian categories accounting for approximately 93 percent of the total.

The 'stateicp' variable encompasses all U.S. states, using their standard abbreviations (e.g., 'CT' for Connecticut), and extends to 55 values to include 'Puerto Rico', 'State groupings (1980 Urban/rural sample)', 'Military/Military Reservations', 'District of Columbia', and an 'State not identified' category.

Age has been grouped into four categories: '18-29', '30-49', '50-64', and '70+'. For educational attainment ('educd'), we've created four categories: 'High school or less', 'Some college', 'College degree', and 'Postgrad'.
 
We excluded any unknown responses to ensure clarity and accuracy in categorization and to enhance clarity and align with survey data, we've renamed 'sex', 'stateicp', 'age', and 'educd' to 'gender', 'state', 'age_group', and 'education', respectively. This restructuring aims to streamline our analysis by ensuring each respondent is accurately categorized.

@fig-distribution-by-age-group, @fig-distribution-by-gender, @fig-distribution-by-race, and @fig-distribution-by-education illustrate comparisons between survey data and post-stratification data across different variables. In these visuals, orange bars represent post-stratification data, while green bars signify survey data. The percentages displayed on each figure are rounded to the nearest tenth, introducing a potential margin of error of Â±0.1% in the total values. Generally, the survey data aligns closely with the post-stratification data, maintaining a discrepancy of about 10% across most categories. Notable exceptions are observed in the '30-49' age group and the 'Some college' education level, where the differences exceed this margin.

```{r}
# insert figure for state here
```

```{r fig.width=10, fig.height=5}
#| echo: false
#| message: false
#| warning: false
#| label: fig-distribution-by-age-group
#| fig-cap: Distribution of Sample and Population by Age Group

# Read in the survey data
survey_data <- cleaned_data_survey

# Read in the post-stratification data
post_strat_data <- cleaned_data_post_strat

# Count the number of occurrences in each age group and calculate percentages for the survey data
survey_counts <- survey_data %>%
  group_by(age_group) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage

# Count the number of occurrences in each age group and calculate percentages for the post-strat data
post_strat_counts <- post_strat_data %>%
  group_by(age_group) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage

# Combine the percentages into a single data frame for plotting
combined_percentages <- rbind(
  data.frame(dataset = 'Survey', age_group = survey_counts$age_group, percentage = survey_counts$percentage),
  data.frame(dataset = 'Post-Strat', age_group = post_strat_counts$age_group, percentage = post_strat_counts$percentage)
)

# Plot the percentages with specified bar width and percentage on y-axis, and label each bar
ggplot(combined_percentages, aes(x = age_group, y = percentage, fill = dataset)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(aes(label = sprintf("%.1f%%", percentage)),
            position = position_dodge(width = 0.8), 
            vjust = -0.3, # Adjust text position
            color = "black",
            size = 3) + 
  labs(x = "Age Group", y = "Proportion", title = "Distribution of Sample and Population by Age Group") +
  theme_minimal() +
  scale_fill_manual(values = c("orange", "green"))
```

```{r fig.width=10, fig.height=5}
#| echo: false
#| message: false
#| warning: false
#| label: fig-distribution-by-gender
#| fig-cap: Distribution of Sample and Population by Gender

# Read in the survey data
survey_data <- cleaned_data_survey

# Read in the post-stratification data
post_strat_data <- cleaned_data_post_strat

# Calculate percentages for the survey data by gender
survey_gender <- survey_data %>%
  group_by(gender) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage

# Calculate percentages for the post-strat data by gender
post_strat_gender <- post_strat_data %>%
  group_by(gender) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage

# Combine the gender percentages into a single data frame for plotting
combined_gender_percentages <- rbind(
  data.frame(dataset = 'Survey', gender = survey_gender$gender, percentage = survey_gender$percentage),
  data.frame(dataset = 'Post-Strat', gender = post_strat_gender$gender, percentage = post_strat_gender$percentage)
)

# Plot the gender percentages with specified bar width and percentage on y-axis, and label each bar
ggplot(combined_gender_percentages, aes(x = gender, y = percentage, fill = dataset)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(aes(label = sprintf("%.1f%%", percentage)),
            position = position_dodge(width = 0.8), 
            vjust = -0.3, # Adjust text position
            color = "black",
            size = 3) + 
  labs(x = "Gender", y = "Proportion", title = "Distribution of Sample and Population by Gender") +
  theme_minimal() +
  scale_fill_manual(values = c("orange", "green"))
```

```{r fig.width=10, fig.height=5}
#| echo: false
#| message: false
#| warning: false
#| label: fig-distribution-by-race
#| fig-cap: Distribution of Sample and Population by Race Ethnicity
# Read in the survey data
survey_data <- cleaned_data_survey
# Read in the post-stratification data
post_strat_data <- cleaned_data_post_strat
# Calculate percentages for the survey data by race
survey_race <- survey_data %>%
  group_by(race) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage
# Calculate percentages for the post-strat data by race
post_strat_race <- post_strat_data %>%
  group_by(race) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage
# Combine the race percentages into a single data frame for plotting
combined_race_percentages <- rbind(
  data.frame(dataset = 'Survey', race = survey_race$race, percentage = survey_race$percentage),
  data.frame(dataset = 'Post-Strat', race = post_strat_race$race, percentage = post_strat_race$percentage)
)
# Adjust the factor levels for the 'race' column to specify the order
combined_race_percentages$race <- factor(combined_race_percentages$race, 
                                                    levels = c("White", "Black", "Asian", "American Indian", "Other"))
# Plot the race percentages with specified bar width and percentage on y-axis, and label each bar
ggplot(combined_race_percentages, aes(x = race, y = percentage, fill = dataset)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(aes(label = sprintf("%.1f%%", percentage)),
            position = position_dodge(width = 0.8), 
            vjust = -0.3, # Adjust text position
            color = "black",
            size = 3) + 
  labs(x = "Race", y = "Proportion", title = "Distribution of Sample and Population by Race Ethnicity") +
  theme_minimal() +
  scale_fill_manual(values = c("orange", "green"))
```

```{r fig.width=10, fig.height=5}
#| echo: false
#| message: false
#| warning: false
#| label: fig-distribution-by-education
#| fig-cap: Distribution of Sample and Population by Education
# Read in the survey data
survey_data <- cleaned_data_survey
# Read in the post-stratification data
post_strat_data <- cleaned_data_post_strat
# Calculate percentages for the survey data by education
survey_education <- survey_data %>%
  group_by(education) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage
# Calculate percentages for the post-strat data by education
post_strat_education <- post_strat_data %>%
  group_by(education) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)  # Calculate percentage
# Combine the education percentages into a single data frame for plotting
combined_education_percentages <- rbind(
  data.frame(dataset = 'Survey', education = survey_education$education, percentage = survey_education$percentage),
  data.frame(dataset = 'Post-Strat', education = post_strat_education$education, percentage = post_strat_education$percentage)
)
# Adjust the factor levels for the 'education' column to specify the order
combined_education_percentages$education <- factor(combined_education_percentages$education, 
                                                    levels = c("High school or less", "Some college", "College degree", "Postgrad"))
# Plot the education percentages with the adjusted order
ggplot(combined_education_percentages, aes(x = education, y = percentage, fill = dataset)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(aes(label = sprintf("%.1f%%", percentage)),
            position = position_dodge(width = 0.8), 
            vjust = -0.3, # Adjust text position
            color = "black",
            size = 3) + 
  labs(x = "Education", y = "Proportion", title = "Distribution of Sample and Population by Education") +
  theme_minimal() +
  scale_fill_manual(values = c("orange", "green"))
```

@tbl-voters-intention-to-vote-for-trump shows the proportion of voters who intend to vote for Donald Trump or not. Also, @tbl-voters-intention-of-their-primary-party shows the proportion of voters' supporting party. The data used to create these table is from the @citeSurveyData. We see that Donald Trump and his party Republican are not expected to win the popular vote before we implement the model.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-voters-intention-to-vote-for-trump
#| tbl-cap: Voters Intention to Support Trump
data <- cleaned_data_survey

# Prepare the data with specified order
trump_counts <- data %>%
  mutate(consider_trump = factor(consider_trump, levels = c("Yes", "No", "Other"))) %>%
  count(consider_trump, name = "Number of Respondents") %>%
  mutate(`Proportion (%)` = (`Number of Respondents` / sum(`Number of Respondents`)) * 100) %>%
  mutate(`Proportion (%)` = round(`Proportion (%)`, 2)) %>%
  rename(`Response` = consider_trump)

# Use kable to create the table
kable(trump_counts, format = "latex", booktabs = TRUE, align = "c") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"), full_width = FALSE)
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-voters-intention-of-their-primary-party
#| tbl-cap: Voters Intention of Their Primary Party

data <- cleaned_data_survey

# Prepare the data with specified order
party_preference_counts <- data %>%
  mutate(primary_party = factor(primary_party, levels = c("Democratic", "Republican", "Other"))) %>%
  count(primary_party, name = "Number of Respondents") %>%
  mutate(`Proportion (%)` = (`Number of Respondents` / sum(`Number of Respondents`)) * 100) %>%
  mutate(`Proportion (%)` = round(`Proportion (%)`, 2)) %>%
  rename(`Party Preference` = primary_party)

# Use kable to create the table
kable(party_preference_counts, format = "latex", booktabs = TRUE, align = "c") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"), full_width = FALSE)
```

# Model
For our study, we employ a technique called multilevel regression with post-stratification (MRP). This approach involves creating a model based on a smaller data set, such as our survey data, and then extending the model's findings to a larger population.

The key steps in MRP involve initially selecting a dataset for model development. In this case, we utilized survey data from the Voter Study Group (@citeSurveyData). The next step is to construct a model with this smaller dataset; here, we employed logistic regression based on the survey data, formulated as seen in equation \ref{eq:logit}. Following model creation, it is then applied to a broader dataset to estimate population characteristics. For our analysis, Census data from IPUMS (@citeIPUMS) served as this larger dataset.

To predict an individual's likelihood of voting for Donald Trump, we aim to construct a logistic regression model leveraging data from the Voter Study Group (@citeSurveyData) and applying post-stratification with Census Data (@citeIPUMS). Given that logistic regression is suited for binary outcomes, we've introduced a variable, 'consider_trump', which assigns a 1 if the respondent indicates a plan to vote for Donald Trump, and a 0 for intentions to vote for other candidates, with 0 encompassing both "No" and "Other" responses.

The logistic regression model takes the form of:

\begin{equation}
log(\frac{\hat{p}}{1 - \hat{p}}) = \beta_{0} + \beta_{1}x_{sex} +
\beta_{2}x_{agegroup} + \beta_{3}x_{race} + \beta_{4}x_{state} + \beta_{5}x_{education}
\label{eq:logit}
\end{equation}

After developing our logistic regression model, we'll utilize the `predict` function in R (@citeR) to apply it to the Census data (@citeIPUMS). This involves segmenting the Census dataset by our target demographic categoriesâsex, race, age_group, education level, and stateâand then executing the model for each group. This process yields probabilities that an individual within a specific group is likely to vote for Donald Trump. With these predictions in hand, we can assess potential outcomes, such as the winner of the popular vote or the distribution of electoral college votes. Additionally, we'll employ a 95 percent confidence interval to estimate the popular vote percentage, indicating a 95 percent certainty level that the true population value falls within our calculated range. This method suggests our findings will have an accuracy margin of +/- 4%.

In the equation \ref{eq:logit}, each $\beta$ is identified as a coefficient to be determined by the regression analysis. The chosen variables for this study include sex, age, race, education, and state, selected primarily because the first three have been recognized as consistent indicators of an individual's preferred candidate. This rationale stems from observed patterns, such as certain states consistently voting for the Republican party, whereas others oscillate between Democratic and Republican preferences in successive elections. Furthermore, we opted for education level over income as a variable, believing that education offers a more definitive characterization of an individual, unlike income.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-model-coefficients
#| tbl-cap: Coefficients from the Model

model <- survey_model

coefficients <- broom.mixed::tidy(model, conf.int = TRUE)

kable(coefficients, format = "latex", booktabs = TRUE, align = "c") %>%
  kable_styling(latex_options = "scale_down", font_size = 7)

```

We are running our regression model using the stan_glm() function in R (@citeR). The decision to run this model over other models like linear regression was made by the fact that we were predicting a binary variable about a voter's decision. Since there are only two possible options our data will likely follow an S shape and a straight line equation will not be helpful to model this relationship. Another strength present for logistic regression is that when combined with post-stratification it allows us to take information from under-represented populations and it allows their views to be accounted for more greatly. For example, our survey data (@citeSurveyData), includes only 7 observations from Alaska, but using multilevel regression with post-stratification, we can have that expanded to over 4500 people.

Our model does have some weaknesses, since the output must be binary, we cannot account for other candidates or a person deciding not to vote. This issue isn't too large because our main goal is to determine which of the two main candidates will be chosen by the people of America. Another weakness we do encounter with our model and multi-level regression with post-stratification is that it has a strong dependence on the survey data. This is a weakness because if the survey has any gaps or there are any tweaks we need to make, it can change the course of results.

@tbl-model-coefficients shows the estimates for the coefficients that will fit into our logistic regression equation. These coefficients will fit into Equation @logit, and were calculated using data from the Voter Study Group (@citeSurveyData). The table is made using `kable` from `knitr` (@citeKnitr) and is formatted using `kableExtra` (@citekableExtra)^[data was cleaned using the `tidy` function called `broom` (@citebroom)]. We can see that our p-values for variables like gender, age, hispanic and education return very significant results while the states have varying levels of significance. What our p-values tell us is whether or not our variable is statistically significant in impacting our response variable, in this case, whether the respondent will vote for Biden or not. When we look at our p-values, we want them to be as small as possible and can assume they're significant if it's under 0.05. As for the coefficients, the value we receive is in terms of log odds. When the log odds are positive, it means that the person will likely vote for Biden and if it's negative it means they'll likely vote for Trump.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-coefficient-estimates
#| fig-cap: Coefficient Estimates

model <- survey_model

coefficients <- broom.mixed::tidy(model, conf.int = TRUE)
# plot our coefficients with error bars to show the upper and lower
# estimates for the coefficients
coefficients %>% ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  labs(title = "Coefficient Estimates",
       x = "Estimate", y = "Coefficient") +
  theme(axis.text.y = element_text(size = 5))
```

@fig-coefficient-estimates shows us the coefficients that would fit into equation \ref{eq:logit} using the polling data (@citeSurveyData). We also have error bars present, which show the upper and lower estimates for the coefficients. What we have to look out for in this scenario is that coefficients with negative values would mean that the person is more likely to vote for Donald Trump (with that characteristic) and positive values mean the person is more likely to vote for Joe Biden. @tbl-model-coefficients shows a numerical view of @fig-coefficient-estimates, along with p-values. 

Using the outputs of the logistic regression model, we can get an equation that follows the form of \ref{eq:logit}, but with the $\beta$ values filled out. This equation is difficult to write out because of the many variables, but in short, if the person's characteristic fits in with a certain variable, it is used in the equation. Then the equation is summed up and the probability is found using equation \@ref{eq:prob}.

We can also look at some common assumptions for logistic regression and check if our model follows them or not. One assumption is that the data set used is large. The data we used to create the model had about 5000 observations, which is high but maybe in the future, we could use a larger data set to increase the accuracy of predictions. Another assumption that logistic regression makes is that the observations are independent of each other. We know that the observations used for our model were independent, not only because the Voter Study Group ensures that there aren't duplicates but because they assign each respondent a unique ID number.

Lastly, using our coefficients (see @tbl-model-coefficients), we can calculate the upper and lower bounds for the probabilities predicted from our model. For our lower probability (white male, from North Dakota, aged 36-49, with some post-secondary education and not hispanic) we get a probability of 22% for supporting Biden. For our upper probability (black woman, from Vermont, aged 18-35, with post-secondary or higher education and considers themselves hispanic) we get a probability of 99% for supporting Joe Biden.

# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)


```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false


```

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows...

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]


```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2


```

\newpage

# References
